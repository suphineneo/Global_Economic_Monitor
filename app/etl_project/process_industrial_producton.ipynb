{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import openpyxl\n",
    "from datetime import datetime, timedelta\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "import logging\n",
    "import time\n",
    "from sqlalchemy import Column, Float, Integer, MetaData, String, Table, create_engine\n",
    "from sqlalchemy.dialects import postgresql\n",
    "from sqlalchemy.engine import URL\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For implementing Data Analysis on `Excel` spreadsheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('/Users/aanwar/Desktop/dec_project_1/Global_Economic_Monitor/app/etl_project/data/Industrial Production, constant 2010 US$, seas. adj..xlsx')\n",
    "df = df.drop(df.index[0])\n",
    "df.rename(columns={'Unnamed: 0': 'Year'}, inplace=True)\n",
    "df.fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing some Data Wrangling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_columns = df.columns[1:]  # Exclude 'Year'\n",
    "\n",
    "# Automate table creation and data insertion for each country\n",
    "for country in country_columns:\n",
    "    # Create a DataFrame for the current country\n",
    "    country_df = df[['Year', country]].copy()\n",
    "    country_df.columns = ['year', 'gdp']  # Rename columns for consistency\n",
    "\n",
    "    # Create a table for the country (if it doesn't exist) and insert data\n",
    "    country_name = country.lower().replace(' ', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering via API Call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My logging function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "\n",
    "def setup_pipeline_logging(pipeline_name: str, log_folder_path: str):\n",
    "    # Initialize logger\n",
    "    logger = logging.getLogger(pipeline_name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Create log file path\n",
    "    file_path = f\"{log_folder_path}/{pipeline_name}_{time.time()}.log\"\n",
    "\n",
    "    # Create handlers\n",
    "    file_handler = logging.FileHandler(file_path)\n",
    "    stream_handler = logging.StreamHandler()\n",
    "\n",
    "    # Set logging levels\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    stream_handler.setLevel(logging.INFO)\n",
    "\n",
    "    # Create formatters and add them to the handlers\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "    file_handler.setFormatter(formatter)\n",
    "    stream_handler.setFormatter(formatter)\n",
    "\n",
    "    # Add handlers to the logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(stream_handler)\n",
    "\n",
    "    return logger, file_path\n",
    "\n",
    "\n",
    "def get_logs(file_path: str) -> str:\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return \"\".join(file.readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postgres Client Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Table, MetaData\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy.dialects import postgresql\n",
    "\n",
    "\n",
    "class PostgreSqlClient:\n",
    "    \"\"\"\n",
    "    A client for querying PostgreSQL database.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        server_name: str,\n",
    "        database_name: str,\n",
    "        username: str,\n",
    "        password: str,\n",
    "        port: int = 5432,\n",
    "    ):\n",
    "        self.host_name = server_name\n",
    "        self.database_name = database_name\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.port = port\n",
    "\n",
    "        connection_url = URL.create(\n",
    "            drivername=\"postgresql+pg8000\",\n",
    "            username=username,\n",
    "            password=password,\n",
    "            host=server_name,\n",
    "            port=port,\n",
    "            database=database_name,\n",
    "        )\n",
    "\n",
    "        self.engine = create_engine(connection_url)\n",
    "\n",
    "    def select_all(self, table: Table) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Select all rows from the specified table.\n",
    "        \"\"\"\n",
    "        with self.engine.connect() as conn:\n",
    "            return [dict(row) for row in conn.execute(table.select()).all()]\n",
    "\n",
    "    def create_table(self, metadata: MetaData) -> None:\n",
    "        \"\"\"\n",
    "        Creates a table defined in the metadata object.\n",
    "        \"\"\"\n",
    "        metadata.create_all(self.engine)\n",
    "\n",
    "    def drop_table(self, table_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Drops a table if it exists.\n",
    "        \"\"\"\n",
    "        with self.engine.connect() as conn:\n",
    "            conn.execute(f\"DROP TABLE IF EXISTS {table_name};\")\n",
    "\n",
    "    def insert(self, data: list[dict], table: Table, metadata: MetaData) -> None:\n",
    "        \"\"\"\n",
    "        Inserts data into a table.\n",
    "        \"\"\"\n",
    "        metadata.create_all(self.engine)\n",
    "        insert_statement = postgresql.insert(table).values(data)\n",
    "        with self.engine.connect() as conn:\n",
    "            conn.execute(insert_statement)\n",
    "\n",
    "    def overwrite(self, data: list[dict], table: Table, metadata: MetaData) -> None:\n",
    "        \"\"\"\n",
    "        Drops the table and recreates it with new data.\n",
    "        \"\"\"\n",
    "        self.drop_table(table.name)\n",
    "        self.insert(data=data, table=table, metadata=metadata)\n",
    "\n",
    "    def upsert(self, data: list[dict], table: Table, metadata: MetaData) -> None:\n",
    "    \n",
    "        \"\"\"\n",
    "        Performs an UPSERT (insert or update on conflict), ensuring no duplicate primary key entries are present in the data.\n",
    "        \"\"\"\n",
    "        \n",
    "        metadata.create_all(self.engine)\n",
    "\n",
    "        # Convert list of dicts to DataFrame to drop duplicates\n",
    "        df = pd.DataFrame(data)\n",
    "        df = df.drop_duplicates(subset=['year', 'country_code'])\n",
    "\n",
    "        # Convert back to list of dicts\n",
    "        data = df.to_dict(orient=\"records\")\n",
    "\n",
    "        key_columns = [pk_column.name for pk_column in table.primary_key.columns.values()]\n",
    "\n",
    "        insert_statement = postgresql.insert(table).values(data)\n",
    "\n",
    "        # Perform conflict resolution: update non-key columns if conflict occurs\n",
    "        upsert_statement = insert_statement.on_conflict_do_update(\n",
    "            index_elements=key_columns,\n",
    "            set_={\n",
    "                col.key: col for col in insert_statement.excluded if col.key not in key_columns\n",
    "            }\n",
    "        )\n",
    "        with self.engine.connect() as conn:\n",
    "            conn.execute(upsert_statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling API calls using incremental loads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-03 21:28:00,691 - worldbankdata_industrial - INFO - Making api connection\n",
      "2024-10-03 21:28:00,691 - worldbankdata_industrial - INFO - Making api connection\n",
      "2024-10-03 21:28:00,691 - worldbankdata_industrial - INFO - Making api connection\n",
      "2024-10-03 21:28:00,691 - worldbankdata_industrial - INFO - Making api connection\n",
      "2024-10-03 21:28:00,691 - worldbankdata_industrial - INFO - Making api connection\n",
      "2024-10-03 21:28:00,691 - worldbankdata_industrial - INFO - Making api connection\n",
      "2024-10-03 21:28:00,694 - worldbankdata_industrial - INFO - Starting to fetch data\n",
      "2024-10-03 21:28:00,694 - worldbankdata_industrial - INFO - Starting to fetch data\n",
      "2024-10-03 21:28:00,694 - worldbankdata_industrial - INFO - Starting to fetch data\n",
      "2024-10-03 21:28:00,694 - worldbankdata_industrial - INFO - Starting to fetch data\n",
      "2024-10-03 21:28:00,694 - worldbankdata_industrial - INFO - Starting to fetch data\n",
      "2024-10-03 21:28:00,694 - worldbankdata_industrial - INFO - Starting to fetch data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-03 21:28:00,691 - worldbankdata_industrial - INFO - Making api connection\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-03 21:28:12,442 - worldbankdata_industrial - INFO - Data has been fetched\n",
      "2024-10-03 21:28:12,442 - worldbankdata_industrial - INFO - Data has been fetched\n",
      "2024-10-03 21:28:12,442 - worldbankdata_industrial - INFO - Data has been fetched\n",
      "2024-10-03 21:28:12,442 - worldbankdata_industrial - INFO - Data has been fetched\n",
      "2024-10-03 21:28:12,442 - worldbankdata_industrial - INFO - Data has been fetched\n",
      "2024-10-03 21:28:12,442 - worldbankdata_industrial - INFO - Data has been fetched\n",
      "2024-10-03 21:28:12,446 - worldbankdata_industrial - INFO - Starting Transformation\n",
      "2024-10-03 21:28:12,446 - worldbankdata_industrial - INFO - Starting Transformation\n",
      "2024-10-03 21:28:12,446 - worldbankdata_industrial - INFO - Starting Transformation\n",
      "2024-10-03 21:28:12,446 - worldbankdata_industrial - INFO - Starting Transformation\n",
      "2024-10-03 21:28:12,446 - worldbankdata_industrial - INFO - Starting Transformation\n",
      "2024-10-03 21:28:12,446 - worldbankdata_industrial - INFO - Starting Transformation\n",
      "2024-10-03 21:28:12,451 - worldbankdata_industrial - INFO - Progressing through my transformations\n",
      "2024-10-03 21:28:12,451 - worldbankdata_industrial - INFO - Progressing through my transformations\n",
      "2024-10-03 21:28:12,451 - worldbankdata_industrial - INFO - Progressing through my transformations\n",
      "2024-10-03 21:28:12,451 - worldbankdata_industrial - INFO - Progressing through my transformations\n",
      "2024-10-03 21:28:12,451 - worldbankdata_industrial - INFO - Progressing through my transformations\n",
      "2024-10-03 21:28:12,451 - worldbankdata_industrial - INFO - Progressing through my transformations\n",
      "2024-10-03 21:28:12,462 - worldbankdata_industrial - INFO - Transformations have been finished\n",
      "2024-10-03 21:28:12,462 - worldbankdata_industrial - INFO - Transformations have been finished\n",
      "2024-10-03 21:28:12,462 - worldbankdata_industrial - INFO - Transformations have been finished\n",
      "2024-10-03 21:28:12,462 - worldbankdata_industrial - INFO - Transformations have been finished\n",
      "2024-10-03 21:28:12,462 - worldbankdata_industrial - INFO - Transformations have been finished\n",
      "2024-10-03 21:28:12,462 - worldbankdata_industrial - INFO - Transformations have been finished\n",
      "2024-10-03 21:28:12,464 - worldbankdata_industrial - INFO - Starting to load into my Postgres\n",
      "2024-10-03 21:28:12,464 - worldbankdata_industrial - INFO - Starting to load into my Postgres\n",
      "2024-10-03 21:28:12,464 - worldbankdata_industrial - INFO - Starting to load into my Postgres\n",
      "2024-10-03 21:28:12,464 - worldbankdata_industrial - INFO - Starting to load into my Postgres\n",
      "2024-10-03 21:28:12,464 - worldbankdata_industrial - INFO - Starting to load into my Postgres\n",
      "2024-10-03 21:28:12,464 - worldbankdata_industrial - INFO - Starting to load into my Postgres\n",
      "2024-10-03 21:28:12,466 - worldbankdata_industrial - INFO - gathering my postgres server info\n",
      "2024-10-03 21:28:12,466 - worldbankdata_industrial - INFO - gathering my postgres server info\n",
      "2024-10-03 21:28:12,466 - worldbankdata_industrial - INFO - gathering my postgres server info\n",
      "2024-10-03 21:28:12,466 - worldbankdata_industrial - INFO - gathering my postgres server info\n",
      "2024-10-03 21:28:12,466 - worldbankdata_industrial - INFO - gathering my postgres server info\n",
      "2024-10-03 21:28:12,466 - worldbankdata_industrial - INFO - gathering my postgres server info\n",
      "2024-10-03 21:28:12,468 - worldbankdata_industrial - INFO - completed gathering my postgres credential info\n",
      "2024-10-03 21:28:12,468 - worldbankdata_industrial - INFO - completed gathering my postgres credential info\n",
      "2024-10-03 21:28:12,468 - worldbankdata_industrial - INFO - completed gathering my postgres credential info\n",
      "2024-10-03 21:28:12,468 - worldbankdata_industrial - INFO - completed gathering my postgres credential info\n",
      "2024-10-03 21:28:12,468 - worldbankdata_industrial - INFO - completed gathering my postgres credential info\n",
      "2024-10-03 21:28:12,468 - worldbankdata_industrial - INFO - completed gathering my postgres credential info\n",
      "2024-10-03 21:28:12,470 - worldbankdata_industrial - INFO - Calling MetaData and creating industrial_data table\n",
      "2024-10-03 21:28:12,470 - worldbankdata_industrial - INFO - Calling MetaData and creating industrial_data table\n",
      "2024-10-03 21:28:12,470 - worldbankdata_industrial - INFO - Calling MetaData and creating industrial_data table\n",
      "2024-10-03 21:28:12,470 - worldbankdata_industrial - INFO - Calling MetaData and creating industrial_data table\n",
      "2024-10-03 21:28:12,470 - worldbankdata_industrial - INFO - Calling MetaData and creating industrial_data table\n",
      "2024-10-03 21:28:12,470 - worldbankdata_industrial - INFO - Calling MetaData and creating industrial_data table\n",
      "2024-10-03 21:28:12,471 - worldbankdata_industrial - INFO - Doing the Load:::::::>>>>>>>\n",
      "2024-10-03 21:28:12,471 - worldbankdata_industrial - INFO - Doing the Load:::::::>>>>>>>\n",
      "2024-10-03 21:28:12,471 - worldbankdata_industrial - INFO - Doing the Load:::::::>>>>>>>\n",
      "2024-10-03 21:28:12,471 - worldbankdata_industrial - INFO - Doing the Load:::::::>>>>>>>\n",
      "2024-10-03 21:28:12,471 - worldbankdata_industrial - INFO - Doing the Load:::::::>>>>>>>\n",
      "2024-10-03 21:28:12,471 - worldbankdata_industrial - INFO - Doing the Load:::::::>>>>>>>\n",
      "2024-10-03 21:28:12,633 - worldbankdata_industrial - INFO - Load ================>>>>>>>100 percent complete\n",
      "2024-10-03 21:28:12,633 - worldbankdata_industrial - INFO - Load ================>>>>>>>100 percent complete\n",
      "2024-10-03 21:28:12,633 - worldbankdata_industrial - INFO - Load ================>>>>>>>100 percent complete\n",
      "2024-10-03 21:28:12,633 - worldbankdata_industrial - INFO - Load ================>>>>>>>100 percent complete\n",
      "2024-10-03 21:28:12,633 - worldbankdata_industrial - INFO - Load ================>>>>>>>100 percent complete\n",
      "2024-10-03 21:28:12,633 - worldbankdata_industrial - INFO - Load ================>>>>>>>100 percent complete\n",
      "2024-10-03 21:28:12,635 - worldbankdata_industrial - INFO - pipeline finished\n",
      "2024-10-03 21:28:12,635 - worldbankdata_industrial - INFO - pipeline finished\n",
      "2024-10-03 21:28:12,635 - worldbankdata_industrial - INFO - pipeline finished\n",
      "2024-10-03 21:28:12,635 - worldbankdata_industrial - INFO - pipeline finished\n",
      "2024-10-03 21:28:12,635 - worldbankdata_industrial - INFO - pipeline finished\n",
      "2024-10-03 21:28:12,635 - worldbankdata_industrial - INFO - pipeline finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting transform for exports\n",
      "Completed transform\n",
      "Data saved successfully to cleaned_export_data.csv\n"
     ]
    }
   ],
   "source": [
    "class WorldBankDataLoader:\n",
    "    def __init__(self, indicator, start_year, end_year):\n",
    "        \"\"\"\n",
    "        Initialize the WorldBankDataLoader with the indicator, start year, and end year.\n",
    "        \n",
    "        Args:\n",
    "        - indicator (str): The indicator code for the World Bank API (e.g., 'NV.IND.TOTL.KD.ZG').\n",
    "        - start_year (str): The starting year for the data.\n",
    "        - end_year (str): The ending year for the data.\n",
    "        \"\"\"\n",
    "        self.indicator = indicator\n",
    "        self.date_range = f\"{start_year}:{end_year}\"\n",
    "        self.base_url = f\"https://api.worldbank.org/v2/countries/all/indicators/{self.indicator}?\"\n",
    "        self.params = {\n",
    "            \"date\": self.date_range,\n",
    "            \"format\": \"json\",\n",
    "            \"page\": 1  # Start at page 1\n",
    "        }\n",
    "        self.all_data = []  # To store paginated data\n",
    "\n",
    "    def fetch_data(self):\n",
    "        \"\"\"\n",
    "        Fetch data from the World Bank API, handling pagination until all data is retrieved.\n",
    "        \n",
    "        Returns:\n",
    "        - pd.DataFrame: A pandas DataFrame containing all the data fetched from the API.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            response = requests.get(self.base_url, params=self.params)\n",
    "            response_data = response.json()\n",
    "\n",
    "            # Check if valid data is returned\n",
    "            if len(response_data) < 2 or not response_data[1]:\n",
    "                break\n",
    "\n",
    "            # Extend the all_data list with the current page's data\n",
    "            self.all_data.extend(response_data[1])\n",
    "\n",
    "            # Increment page number for the next API call\n",
    "            self.params[\"page\"] += 1\n",
    "\n",
    "        # Normalize and return the data as a pandas DataFrame\n",
    "        return pd.json_normalize(data=self.all_data)\n",
    "    \n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Clean and filter the data for specific countries, remove NaN values, \n",
    "        and save the result to a CSV file.\n",
    "        \n",
    "        Args:\n",
    "        - df (pd.DataFrame): The DataFrame to be transformed.\n",
    "        \n",
    "        Returns:\n",
    "        - pd.DataFrame: The cleaned and filtered DataFrame.\n",
    "        \"\"\"\n",
    "        print(\"Starting transform for exports\")\n",
    "\n",
    "        df_selected = df[\n",
    "            [\n",
    "                \"date\",\n",
    "                \"countryiso3code\",\n",
    "                \"country.value\",\n",
    "                \"indicator.id\",\n",
    "                \"indicator.value\",\n",
    "                \"value\",\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        df_renamed = df_selected.rename(\n",
    "            columns={\n",
    "                \"date\": \"year\",\n",
    "                \"countryiso3code\": \"country_code\",\n",
    "                \"country.value\": \"country_name\",\n",
    "                \"indicator.id\": \"indicator_id\",\n",
    "                \"indicator.value\": \"indicator_value\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Remove NaN from the Year and value column\n",
    "        df_cleaned = df_renamed.dropna(subset=[\"year\"]).dropna(subset=[\"value\"])\n",
    "\n",
    "        df_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        df_cleaned = df_cleaned.astype({\"year\": int})\n",
    "\n",
    "        print(\"Completed transform\")\n",
    "\n",
    "        try:\n",
    "            df_cleaned.to_csv(\"data/industrial_cleaned_export_data.csv\", index=False)\n",
    "            print(\"Data saved successfully to cleaned_export_data.csv\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data to CSV: {e}\")\n",
    "\n",
    "        return df_cleaned\n",
    "    \n",
    "    def load(self, df: pd.DataFrame, postgresql_client, table: Table, metadata: MetaData, load_method: str = \"overwrite\"):\n",
    "        \"\"\"\n",
    "        Load the cleaned data into a PostgreSQL database.\n",
    "        \n",
    "        Args:\n",
    "        - df (pd.DataFrame): The cleaned DataFrame to be loaded.\n",
    "        - postgresql_client: A PostgreSQL client instance to execute the load.\n",
    "        - table (Table): The SQLAlchemy Table object representing the target table.\n",
    "        - metadata (MetaData): The SQLAlchemy MetaData object representing the schema.\n",
    "        - load_method (str): The method for loading data: 'insert', 'upsert', or 'overwrite'.\n",
    "        \"\"\"\n",
    "        # Create the upsert statement based on the load method\n",
    "        if load_method == \"insert\":\n",
    "            postgresql_client.insert(\n",
    "                data=df.to_dict(orient=\"records\"), table=table, metadata=metadata\n",
    "            )\n",
    "        elif load_method == \"upsert\":\n",
    "            postgresql_client.upsert(\n",
    "                data=df.to_dict(orient=\"records\"), table=table, metadata=metadata\n",
    "            )\n",
    "        elif load_method == \"overwrite\":\n",
    "            postgresql_client.overwrite(\n",
    "                data=df.to_dict(orient=\"records\"), table=table, metadata=metadata\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Please specify a correct load method: [insert, upsert, overwrite]\"\n",
    "            )\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    load_dotenv()\n",
    "    DB_USERNAME = os.getenv(\"DB_USERNAME\")\n",
    "    DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "    SERVER_NAME = os.getenv(\"SERVER_NAME\")\n",
    "    DATABASE_NAME = os.getenv(\"DATABASE_NAME\")\n",
    "    PORT = os.environ.get(\"PORT\")\n",
    "    # Create an instance of the loader for the specific indicator and date range\n",
    "    loader = WorldBankDataLoader(indicator=\"NV.IND.TOTL.KD.ZG\", start_year=\"2020\", end_year=\"2024\")\n",
    "\n",
    "    logger, log_file = setup_pipeline_logging(\"worldbankdata_industrial\", \"logs\")\n",
    "    logger.info(\"Making api connection\")\n",
    "    logs = get_logs(log_file)\n",
    "    print(logs)\n",
    "\n",
    "    # Fetch the data\n",
    "    logger.info(\"Starting to fetch data\")\n",
    "    df_data = loader.fetch_data()\n",
    "    logger.info(\"Data has been fetched\")\n",
    "\n",
    "    logger.info(\"Starting Transformation\")\n",
    "    logger.info(\"Progressing through my transformations\")\n",
    "    cleaned_df = loader.transform(df_data)\n",
    "    logger.info(\"Transformations have been finished\")\n",
    "\n",
    "    logger.info(\"Starting to load into my Postgres\")\n",
    "    logger.info(\"gathering my postgres server info\")\n",
    "    postgresql_client = PostgreSqlClient(\n",
    "    server_name=SERVER_NAME,\n",
    "    database_name=DATABASE_NAME,\n",
    "    username=DB_USERNAME,\n",
    "    password=DB_PASSWORD,\n",
    "    port=PORT,\n",
    "    )\n",
    "    logger.info(\"completed gathering my postgres credential info\")\n",
    "    logger.info(\"Calling MetaData and creating industrial_data table\")\n",
    "    metadata = MetaData()\n",
    "    export_table = Table(\n",
    "        \"industrial_data\",\n",
    "        metadata,\n",
    "        Column(\"year\", Integer, primary_key=True),\n",
    "        Column(\"country_code\", String, primary_key = True),\n",
    "        Column(\"country_name\", String),\n",
    "        Column(\"indicator_id\", String),\n",
    "        Column(\"indicator_value\", String),\n",
    "        Column(\"value\", Float),\n",
    "    )\n",
    "    logger.info(\"Doing the Load:::::::>>>>>>>\")\n",
    "    loader.load(\n",
    "        cleaned_df,\n",
    "        postgresql_client=postgresql_client,\n",
    "        table=export_table,\n",
    "        metadata=metadata,\n",
    "        load_method=\"upsert\"\n",
    "    ) \n",
    "    logger.info(\"Load ================>>>>>>>100 percent complete\")\n",
    "    logger.info(\"pipeline finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>countryiso3code</th>\n",
       "      <th>date</th>\n",
       "      <th>value</th>\n",
       "      <th>unit</th>\n",
       "      <th>obs_status</th>\n",
       "      <th>decimal</th>\n",
       "      <th>indicator.id</th>\n",
       "      <th>indicator.value</th>\n",
       "      <th>country.id</th>\n",
       "      <th>country.value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFE</td>\n",
       "      <td>2023</td>\n",
       "      <td>1.956700</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>NV.IND.TOTL.KD.ZG</td>\n",
       "      <td>Industry (including construction), value added...</td>\n",
       "      <td>ZH</td>\n",
       "      <td>Africa Eastern and Southern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFE</td>\n",
       "      <td>2022</td>\n",
       "      <td>2.693175</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>NV.IND.TOTL.KD.ZG</td>\n",
       "      <td>Industry (including construction), value added...</td>\n",
       "      <td>ZH</td>\n",
       "      <td>Africa Eastern and Southern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFE</td>\n",
       "      <td>2021</td>\n",
       "      <td>4.335020</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>NV.IND.TOTL.KD.ZG</td>\n",
       "      <td>Industry (including construction), value added...</td>\n",
       "      <td>ZH</td>\n",
       "      <td>Africa Eastern and Southern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFE</td>\n",
       "      <td>2020</td>\n",
       "      <td>-4.734432</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>NV.IND.TOTL.KD.ZG</td>\n",
       "      <td>Industry (including construction), value added...</td>\n",
       "      <td>ZH</td>\n",
       "      <td>Africa Eastern and Southern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFW</td>\n",
       "      <td>2023</td>\n",
       "      <td>1.744502</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>NV.IND.TOTL.KD.ZG</td>\n",
       "      <td>Industry (including construction), value added...</td>\n",
       "      <td>ZI</td>\n",
       "      <td>Africa Western and Central</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  countryiso3code  date     value unit obs_status  decimal       indicator.id  \\\n",
       "0             AFE  2023  1.956700                        1  NV.IND.TOTL.KD.ZG   \n",
       "1             AFE  2022  2.693175                        1  NV.IND.TOTL.KD.ZG   \n",
       "2             AFE  2021  4.335020                        1  NV.IND.TOTL.KD.ZG   \n",
       "3             AFE  2020 -4.734432                        1  NV.IND.TOTL.KD.ZG   \n",
       "4             AFW  2023  1.744502                        1  NV.IND.TOTL.KD.ZG   \n",
       "\n",
       "                                     indicator.value country.id  \\\n",
       "0  Industry (including construction), value added...         ZH   \n",
       "1  Industry (including construction), value added...         ZH   \n",
       "2  Industry (including construction), value added...         ZH   \n",
       "3  Industry (including construction), value added...         ZH   \n",
       "4  Industry (including construction), value added...         ZI   \n",
       "\n",
       "                 country.value  \n",
       "0  Africa Eastern and Southern  \n",
       "1  Africa Eastern and Southern  \n",
       "2  Africa Eastern and Southern  \n",
       "3  Africa Eastern and Southern  \n",
       "4   Africa Western and Central  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1064 entries, 0 to 1063\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   countryiso3code  1064 non-null   object \n",
      " 1   date             1064 non-null   object \n",
      " 2   value            887 non-null    float64\n",
      " 3   unit             1064 non-null   object \n",
      " 4   obs_status       1064 non-null   object \n",
      " 5   decimal          1064 non-null   int64  \n",
      " 6   indicator.id     1064 non-null   object \n",
      " 7   indicator.value  1064 non-null   object \n",
      " 8   country.id       1064 non-null   object \n",
      " 9   country.value    1064 non-null   object \n",
      "dtypes: float64(1), int64(1), object(8)\n",
      "memory usage: 83.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_2 = pd.read_csv(\"data/cleaned_export_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 887 entries, 0 to 886\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   year             887 non-null    int64  \n",
      " 1   country_code     872 non-null    object \n",
      " 2   country_name     887 non-null    object \n",
      " 3   indicator_id     887 non-null    object \n",
      " 4   indicator_value  887 non-null    object \n",
      " 5   value            887 non-null    float64\n",
      "dtypes: float64(1), int64(1), object(4)\n",
      "memory usage: 41.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_cleaned_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abdullahs-etl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
